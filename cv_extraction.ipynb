{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHOInXQZyovb",
        "outputId": "5d8d0bc3-65b9-4fa9-8401-1db4a5589bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tika\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJNgrR2Ey40l",
        "outputId": "f86e3bb7-c6d1-4e79-8c1c-967d6a9e1010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tika\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tika) (67.7.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tika) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2024.2.2)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32621 sha256=60347db3f73ce5b3e0447012220fdf9ad4df830ffcd31e1ee6baacabea0a6303\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tika import parser\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "o93QbIiizTlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extracting Text Data from CVs**"
      ],
      "metadata": {
        "id": "pu5pPJT6FWvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess a CV file\n",
        "def preprocess_cv(cv_path):\n",
        "    # Parse the CV file using Apache Tika\n",
        "    parsed = parser.from_file(cv_path)\n",
        "\n",
        "    # Extract text content and metadata\n",
        "    text_content = parsed['content']\n",
        "    metadata = parsed['metadata']\n",
        "\n",
        "    # Extract relevant metadata fields (e.g., author, creation date, etc.)\n",
        "    author = metadata.get('Author', 'Unknown')\n",
        "    creation_date = metadata.get('Creation-Date', 'Unknown')\n",
        "\n",
        "    # Create key-value pairs for the extracted metadata and text content\n",
        "    key_value_pairs = {\n",
        "        'Text Content': text_content\n",
        "    }\n",
        "\n",
        "    return key_value_pairs\n",
        "\n",
        "# Directory containing the CV files in Google Drive\n",
        "cv_directory = '/content/drive/MyDrive/Piramal Hackathon/dataset/CV test'\n",
        "\n",
        "# Preprocess each CV file in the directory\n",
        "cv_key_value_pairs = {}\n",
        "for filename in os.listdir(cv_directory):\n",
        "    if filename.endswith('.docx'):\n",
        "        cv_path = os.path.join(cv_directory, filename)\n",
        "        cv_key_value_pairs[filename] = preprocess_cv(cv_path)\n",
        "\n",
        "# Save the key-value pairs to a JSON file\n",
        "output_file = '/content/drive/MyDrive/Piramal Hackathon/cv_pairs.json'\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(cv_key_value_pairs, f, indent=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "fzTSaD2DzXFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5376374c-a9b3-480e-fe35-63e835ee7a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-11 06:00:34,143 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /tmp/tika-server.jar.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /tmp/tika-server.jar.\n",
            "2024-04-11 06:00:34,423 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2024-04-11 06:00:34,646 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
            "WARNING:tika.tika:Failed to see startup log message; retrying...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning of text from unwanted spaces,characters**"
      ],
      "metadata": {
        "id": "tU-FOnYvFdDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_text(text):\n",
        "    # Remove newline characters and extra spaces, and return only words\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def clean_json(cv_key_value_pairs):\n",
        "    cleaned_data = {}\n",
        "    for filename, entry in cv_key_value_pairs.items():\n",
        "        text = entry['Text Content']\n",
        "\n",
        "        # Clean text content\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', text.replace('\\n', ' ')).strip()\n",
        "\n",
        "        # Update entry with cleaned text content\n",
        "        cleaned_entry = {\n",
        "            'Text Content': cleaned_text\n",
        "        }\n",
        "        cleaned_data[filename] = cleaned_entry\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "# Read the pairs from the JSON file\n",
        "input_file = '/content/drive/MyDrive/Piramal Hackathon/cv_pairs.json'\n",
        "with open(input_file, 'r') as f:\n",
        "    cv_key_value_pairs = json.load(f)\n",
        "\n",
        "# Clean the text content while keeping the format\n",
        "cleaned_data = clean_json(cv_key_value_pairs)\n",
        "\n",
        "# Save the cleaned data to a new JSON file\n",
        "output_file = '/content/drive/MyDrive/Piramal Hackathon/cleaned_pairs1.json'\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(cleaned_data, f, indent=4)\n",
        "\n",
        "print(\"Cleaning complete. The cleaned data has been saved to\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8hGRuBYzrmV",
        "outputId": "e1618fe0-8ed6-424d-e182-38f03059bc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning complete. The cleaned data has been saved to /content/drive/MyDrive/Piramal Hackathon/cleaned_pairs1.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove newline characters, extra spaces, and return only words\n",
        "    cleaned_text = ' '.join(text.split())\n",
        "    # Remove Unicode characters\n",
        "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', cleaned_text)\n",
        "    # Remove colons (':') and dashes ('-')\n",
        "    cleaned_text = cleaned_text.replace(':', '').replace('-', '')\n",
        "    # Remove extra spaces\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
        "    return cleaned_text\n",
        "\n",
        "def clean_json(cv_key_value_pairs):\n",
        "    cleaned_data = {}\n",
        "    for filename, entry in cv_key_value_pairs.items():\n",
        "        text = entry['Text Content']\n",
        "\n",
        "        # Clean text content\n",
        "        cleaned_text = clean_text(text)\n",
        "\n",
        "        # Update entry with cleaned text content\n",
        "        cleaned_entry = {\n",
        "            'Text Content': cleaned_text\n",
        "        }\n",
        "        cleaned_data[filename] = cleaned_entry\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "# Read the pairs from the JSON file\n",
        "input_file = '/content/drive/MyDrive/Piramal Hackathon/cleaned_pairs1.json'\n",
        "with open(input_file, 'r') as f:\n",
        "    cv_key_value_pairs = json.load(f)\n",
        "\n",
        "# Clean the text content while keeping the format\n",
        "cleaned_data = clean_json(cv_key_value_pairs)\n",
        "\n",
        "# Save the cleaned data to a new JSON file\n",
        "output_file = '/content/drive/MyDrive/Piramal Hackathon/cleaned_pairs2.json'\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(cleaned_data, f, indent=4)\n",
        "\n",
        "print(\"Cleaning complete. The cleaned data has been saved to\", output_file)\n"
      ],
      "metadata": {
        "id": "PZ8ZOqLb-fJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0536303-7598-4998-c501-261a2dfd7942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning complete. The cleaned data has been saved to /content/drive/MyDrive/Piramal Hackathon/cleaned_pairs2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Converting json to xlsx**"
      ],
      "metadata": {
        "id": "V6v2Y4CAz0-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the JSON file into a DataFrame\n",
        "input_file = '/content/drive/MyDrive/Piramal Hackathon/cleaned_pairs2.json'\n",
        "df = pd.read_json(input_file)\n",
        "\n",
        "# Define the output Excel file name\n",
        "output_file = '/content/drive/MyDrive/Piramal Hackathon/employees.xlsx'\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Conversion complete. The Excel file has been saved to\", output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p9b26D1ySAz",
        "outputId": "53d8406b-b840-454a-fe14-4c284341eaf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion complete. The Excel file has been saved to /content/drive/MyDrive/Piramal Hackathon/employees.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing unwanted characters and spaces**"
      ],
      "metadata": {
        "id": "8V6yfHedz7NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the Excel file\n",
        "df = pd.read_excel('/content/drive/MyDrive/Piramal Hackathon/employees.xlsx')\n",
        "\n",
        "# Remove text patterns like 'imageXX.png' or 'imageXXX.png'\n",
        "df = df.replace(to_replace=r'image\\d+\\.png', value='', regex=True)\n",
        "\n",
        "# Remove the occurrence of '}\\n'\n",
        "df = df.replace(to_replace=r'}\\n', value='', regex=True)\n",
        "\n",
        "# Remove any extra spaces\n",
        "df = df.replace(to_replace=r'\\s+', value=' ', regex=True)\n",
        "\n",
        "# Save the cleaned data back to the Excel file\n",
        "df.to_excel('/content/drive/MyDrive/Piramal Hackathon/cleaned_excel_file.xlsx',index=False)"
      ],
      "metadata": {
        "id": "DEk41K1z_4Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OlB2qXQsypQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}